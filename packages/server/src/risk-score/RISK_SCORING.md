# Risk Scoring System

This document explains how EasyCommunitySpamBlocker calculates risk scores for publications.

## Overview

The risk score is a value between 0.0 and 1.0 that indicates the likelihood a publication is spam or malicious:

- **0.0 - 0.3**: Low risk (likely legitimate)
- **0.3 - 0.7**: Moderate risk (may require verification)
- **0.7 - 1.0**: High risk (likely spam/malicious)

The score is calculated as a weighted combination of multiple factors, each analyzing different aspects of the publication and its author.

## Trust Model

**Important**: Not all author data can be trusted equally.

- `author.subplebbit.*` fields are **PARTIALLY TRUSTED** - they are generated by the subplebbit, but subplebbit owners can manipulate them
- All other `author.*` fields are **UNTRUSTED** - they are provided by the user and can be faked

**Note on `author.wallets` and `author.avatar`**: These fields contain internal EIP-191 signatures that prove wallet/NFT ownership. The `/evaluate` endpoint **validates these signatures** before processing:

- **Wallet signatures**: Verified using viem's `verifyMessage()` against the claimed wallet address
- **Domain wallet addresses** (e.g., ENS): Verified that the `plebbit-author-address` TXT record matches the publication's signing key
- **Avatar (NFT) signatures**: Verified against the **current NFT owner** by querying the blockchain

If wallet or avatar signature verification fails, the publication is **rejected** by the `/evaluate` endpoint. Therefore, these fields can be **TRUSTED** for risk scoring purposes once the publication passes verification.

The risk scoring system primarily relies on `author.subplebbit` data for reputation signals, with mitigations for manipulation.

**Note on `author.subplebbit` trust limitations**: While `author.subplebbit.*` fields are generated by the subplebbit (not the user), a malicious subplebbit owner can still manipulate values like `postScore` and `replyScore`. The count-based karma approach mitigates this by giving each subplebbit exactly one vote regardless of karma magnitude. Additionally, an author's `author.subplebbit` data only exists after they sign and publish a comment to that subplebbit—this means attackers cannot create thousands of subplebbits to manipulate _someone else's_ reputation, since the target must actively participate in each sub.

**Note on timestamp trust**: `author.subplebbit.firstCommentTimestamp` and `comment.timestamp` are **NOT TRUSTED** for account age calculation. A malicious subplebbit owner can fabricate `firstCommentTimestamp`, and since subplebbits only validate that `comment.timestamp` is "recent", an owner can backdate their own comments. We cannot detect subplebbit ownership at the protocol level (author signer keys are separate from subplebbit keys). Therefore, account age is calculated **only from server-generated timestamps** (when our system first observed the author).

## Author Identity Tracking

**Important**: `author.address` is NOT used for identity tracking.

The `author.address` field can be either:

- A b58-encoded IPNS address (cryptographically tied to the author's key)
- A domain name (not cryptographically tied to the author)

Since `author.address` can be a domain that the author controls but doesn't cryptographically prove ownership of, we use the **signature's public key** (`publication.signature.publicKey`) for all identity-related tracking:

- Velocity tracking (how fast an author is posting)
- Karma aggregation (author's reputation across subplebbits)
- Account age (first seen timestamp)
- Content similarity detection (same author vs. different authors)
- Link spam detection (same author vs. coordinated campaigns)

The Ed25519 public key in the signature is the cryptographic identifier that truly identifies the author across all their publications.

## Data Sources

Risk factors query data from two separate database table sets:

1. **Engine tables** (`comments`, `votes`, `commentEdits`, `commentModerations`): Populated by `/evaluate` endpoint when publications are submitted for spam detection
2. **Indexer tables** (`indexed_comments_ipfs`, `indexed_comments_update`, `modqueue_*`): Populated by the background indexer which crawls subplebbits

The `CombinedDataService` queries both sources and combines data using factor-specific strategies:

| Factor                  | Combination Strategy                                            |
| ----------------------- | --------------------------------------------------------------- |
| Account Age             | MIN of server-generated timestamps only (receivedAt, fetchedAt) |
| Karma                   | Per-subplebbit, use LATEST entry from either source             |
| Velocity                | SUM (combine counts from both sources)                          |
| Content/Link Similarity | UNION (query both sources)                                      |
| Network factors         | Indexer only (ban history, modqueue rejection, removal)         |

This approach provides:

- **Broader coverage**: Authors may have history in indexer even if they've never used this spam detection server
- **Fresher data**: Engine data is real-time from `/evaluate` calls
- **More accurate**: Uses the most relevant data for each factor type

## Risk Factors

### 1. Account Age (Weight: 12% without IP, 8% with IP)

Evaluates how long the author has been known to our system, using **only server-generated timestamps**:

1. `receivedAt` from engine database (when our server received the publication)
2. `fetchedAt` from indexer database (when our indexer fetched the comment)

The oldest timestamp from either source is used.

**SECURITY**: We do **NOT** trust:

- `author.subplebbit.firstCommentTimestamp` - can be fabricated by malicious subplebbit owners
- `comment.timestamp` - subplebbit owners can backdate their own comments (subplebbits only validate timestamps are "recent", which owners can bypass)

We cannot detect subplebbit ownership at the protocol level (author signer keys are separate from subplebbit keys), so we cannot filter "owned" vs "external" subplebbits. The only secure approach is to trust only our own server-generated timestamps.

**Tradeoff**: Account age reflects how long the author has been known to our system, not their actual Plebbit history. This is acceptable for security.

| Account Age | Risk Score | Description                      |
| ----------- | ---------- | -------------------------------- |
| > 365 days  | 0.10       | Very established, highly trusted |
| > 90 days   | 0.20       | Established account              |
| > 30 days   | 0.35       | Moderately established           |
| > 7 days    | 0.50       | Some history                     |
| > 1 day     | 0.70       | New account                      |
| < 1 day     | 0.85       | Very new account                 |
| No history  | 1.0        | No history in our system         |

**Rationale**: Older accounts (as observed by our system) have demonstrated sustained, non-malicious behavior over time.

### 2. Karma Score (Weight: 10% without IP, 6% with IP)

Evaluates the author's reputation using a **count-based approach** that is resistant to collusion attacks.

Instead of using raw karma values (which can be manipulated by colluding subplebbits), we count how many subplebbits the author has positive vs negative karma in. Each subplebbit gets exactly **1 vote** regardless of karma magnitude.

**Why count-based?**

- **Collusion resistance**: A few hostile subs giving massive negative karma can't tank an author's score if they have good standing elsewhere
- **Sybil resistance**: Inflated positive karma from friendly subs matters less when you're just counting "how many subs trust this author"
- **Democratic**: Each sub gets equal weight in the reputation signal

**Data sources:**

- **Current subplebbit**: From `author.subplebbit` (TRUSTED) in the challenge request
- **Other subplebbits**: Aggregated from combined engine + indexer data, using the **latest** entry per subplebbit

**Counting logic:**

- Positive sub: `postScore + replyScore > 0` → +1 positive vote
- Negative sub: `postScore + replyScore < 0` → +1 negative vote
- Zero karma: Doesn't count either way
- Net count = (positive subs) - (negative subs)

| Net Sub Count | Risk Score | Description                      |
| ------------- | ---------- | -------------------------------- |
| >= +5         | 0.10       | Widely trusted across network    |
| +3 to +4      | 0.20       | Trusted in multiple communities  |
| +1 to +2      | 0.35       | Generally positive standing      |
| 0             | 0.50       | Mixed/balanced reputation        |
| -1 to -2      | 0.65       | Some concerns                    |
| -3 to -4      | 0.80       | Multiple communities flag issues |
| <= -5         | 0.90       | Widely mistrusted                |
| No data       | 0.60       | Unknown author (slight negative) |

**Example - Collusion resistance:**

An author has:

- +10 karma in sub-a.eth (1 positive vote)
- +20 karma in sub-b.eth (1 positive vote)
- -1000 karma in hostile-sub.eth (1 negative vote)

Net = 2 - 1 = +1 → Risk score 0.35 (generally positive)

The hostile sub's massive negative karma only counts as 1 negative vote, not enough to override the author's good standing in other communities.

**Rationale**: This approach asks "How many independent communities vouch for this author?" which is a more robust signal than raw karma sums that can be gamed by a single bad actor.

**Mitigation - Domain-only karma**: To prevent self-promotion attacks, only karma from **domain-addressed subplebbits** is counted. IPNS addresses (like `12D3KooW...`) are free to create, making them vulnerable to sybil attacks. Domain addresses (like `example.eth`, `example.sol`, `example.com`) cost money to acquire, creating an economic barrier that makes mass subplebbit creation impractical. Additionally, the `/evaluate` endpoint only accepts requests from domain-addressed subplebbits that can be resolved via plebbit-js.

### 3. Comment Content/Title Risk (Weight: 14% without IP, 10% with IP)

Analyzes comment content and title for spam indicators by querying both engine and indexer databases for similar past publications. **Only applies to comments (posts and replies)** - returns neutral score (0.5) for other publication types.

**Similarity detection uses:**

1. SQL substring matching (LIKE) to find candidate matches
2. Jaccard similarity on word sets to calculate actual similarity (threshold: 60%)

**Note**: Results from both engine and indexer tables are combined to detect cross-subplebbit spam patterns.

| Indicator                                      | Score Impact | Description                            |
| ---------------------------------------------- | ------------ | -------------------------------------- |
| **Same Author - Content Duplicates**           |              |                                        |
| 5+ duplicate comments from same author in 24h  | +0.35        | Heavy self-spamming                    |
| 3-4 duplicate comments from same author in 24h | +0.25        | Moderate self-spamming                 |
| 1-2 duplicate comments from same author in 24h | +0.15        | Possible duplicate posting             |
| **Same Author - Similar Content**              |              |                                        |
| 3+ similar comments from same author in 24h    | +0.20        | Similar content spamming               |
| 1-2 similar comments from same author in 24h   | +0.10        | Possible template spam                 |
| **Other Authors - Content Duplicates**         |              |                                        |
| 5+ identical comments from other authors       | +0.40        | Coordinated spam campaign              |
| 2-4 identical comments from other authors      | +0.25        | Possible coordinated activity          |
| 1 identical comment from another author        | +0.10        | Content seen from another author       |
| **Other Authors - Similar Content**            |              |                                        |
| 3+ similar comments from other authors         | +0.20        | Similar content from multiple accounts |
| 1-2 similar comments from other authors        | +0.08        | Similar content seen elsewhere         |
| **Same Author - Title Duplicates**             |              |                                        |
| 3+ posts with same title from author in 24h    | +0.30        | Title spam                             |
| 1-2 posts with same title from author in 24h   | +0.15        | Possible duplicate posting             |
| **Same Author - Similar Titles**               |              |                                        |
| 2+ posts with similar title from author in 24h | +0.15        | Similar title spamming                 |
| **Other Authors - Title Duplicates**           |              |                                        |
| 3+ posts with same title from other authors    | +0.25        | Coordinated title spam                 |
| 1-2 posts with same title from another author  | +0.10        | Title seen from another author         |
| **Other Authors - Similar Titles**             |              |                                        |
| 2+ posts with similar title from other authors | +0.10        | Similar titles from multiple accounts  |
| **Static Content Analysis**                    |              |                                        |
| 5+ URLs in content                             | +0.15        | Excessive link dropping                |
| 3-4 URLs in content                            | +0.08        | Elevated link count                    |
| Excessive capitalization (>50% caps)           | +0.08        | SHOUTING (spam indicator)              |
| Repetitive patterns (repeated chars/words)     | +0.10        | Bot-like behavior                      |

**Note**: The base score for comments starts at 0.2 (low risk). Non-comment publications receive a neutral score of 0.5.

### 4. Comment URL/Link Risk (Weight: 12% without IP, 10% with IP)

Analyzes URLs from multiple sources in comments for spam indicators by querying both engine and indexer databases. **Only applies to comments (posts and replies)** - returns neutral score (0.5) for other publication types.

**URL Sources:**

- `comment.link` - dedicated link field for link posts
- `comment.content` - URLs embedded in post/reply content
- `comment.title` - URLs in post titles

All URLs from these sources are extracted, normalized, and analyzed for spam patterns.

Link counts from both sources are **summed** to detect cross-subplebbit link spam campaigns.

**No Fixed Time Window:**

Unlike some spam detection systems that only look at the last 24 hours, this factor analyzes the **entire historical record** of URL patterns. The key signal for distinguishing spam from organic behavior is **time clustering** - how closely together posts are made, not an arbitrary cutoff.

| Indicator                                          | Score Impact   | Description                                |
| -------------------------------------------------- | -------------- | ------------------------------------------ |
| **Same Author - Duplicate Links**                  |                |                                            |
| 5+ posts with same URL from author                 | +0.40          | Heavy link spam                            |
| 3-4 posts with same URL from author                | +0.25          | Moderate link spam                         |
| 1-2 posts with same URL from author                | +0.15          | Possible duplicate posting                 |
| **Other Authors - Duplicate Links (Coordinated)**  |                |                                            |
| 10+ posts with same URL from other authors         | +0.50          | Likely coordinated spam campaign           |
| 5-9 posts with same URL from other authors         | +0.35          | Possible coordinated spam                  |
| 2-4 posts with same URL from other authors         | +0.20          | URL seen from multiple authors             |
| 1 post with same URL from another author           | +0.10          | URL seen from another author               |
| **Domain Spam (Same Author)**                      |                |                                            |
| 10+ links to same domain from author               | +0.25          | Domain-focused spam                        |
| 5-9 links to same domain from author               | +0.15          | Elevated domain promotion                  |
| **Similar URL Detection (Prefix Matching)**        |                |                                            |
| 3+ similar URLs from author (clustered)            | +0.25 to +0.65 | URL variation spam (see time clustering)   |
| 3+ similar URLs from author (spread out)           | +0.10 to +0.20 | Lower risk if spread over time             |
| 5+ similar URLs from 3+ other authors (clustered)  | +0.30 to +0.60 | Coordinated campaign (see time clustering) |
| 5+ similar URLs from 3+ other authors (spread out) | +0.15          | Organic sharing pattern                    |
| **Suspicious URL Patterns**                        |                |                                            |
| Uses IP address instead of domain                  | +0.20          | Suspicious direct IP link                  |

**URL Similarity Detection:**

To catch spammers who slightly modify URLs (e.g., changing query parameters or referral codes), we extract a **URL prefix** consisting of `domain/path1/path2` (first two path segments). URLs with the same prefix are considered "similar."

**Example:** `https://spam.com/promo/deal?ref=abc` and `https://spam.com/promo/deal?ref=xyz` both have prefix `spam.com/promo/deal` and would be flagged as similar.

**Time Clustering Analysis:**

Time clustering is used to distinguish coordinated spam bursts from organic sharing that happens over days/weeks/months. We analyze the **standard deviation** of posting timestamps (using `publication.timestamp`) to measure how clustered posts are:

| Time Clustering (stddev) | Additional Risk | Description                                |
| ------------------------ | --------------- | ------------------------------------------ |
| < 1 hour                 | +0.30           | Very tight clustering - likely coordinated |
| 1-3 hours                | +0.20           | Moderate clustering - suspicious           |
| 3-6 hours                | +0.10           | Some clustering                            |
| > 6 hours                | +0.00           | Spread out - likely organic sharing        |

**Time clustering is applied to both same-author and cross-author URL detection:**

- **Same-author**: Rapid-fire posting of similar URLs gets higher risk than occasional reposting
- **Cross-author**: Multiple authors posting similar URLs within a short time indicates coordination

**Example - Cross-author:** 5 posts with the same URL prefix from 5 different authors:

- Posted within 10 minutes of each other → stddev ≈ 3 minutes → base +0.30, clustering +0.30 = +0.60 total risk
- Posted over 20 hours → stddev ≈ 7 hours → +0.15 risk (spread out, likely organic)

**Example - Same-author:** An author posts 5 similar URLs:

- All within 30 minutes → stddev ≈ 10 minutes → base +0.35, clustering +0.30 = +0.65 total risk
- Spread over 2 weeks → stddev > 24 hours → +0.20 risk (occasional sharing, not spam burst)

**Similarity Allowlist:**

Popular platforms where URL paths naturally vary (different content, not spam variations) are **excluded from similarity detection** but still checked for exact URL matches:

- Social media: x.com, twitter.com, youtube.com, youtu.be, reddit.com, facebook.com, instagram.com, tiktok.com, linkedin.com
- Developer platforms: github.com, gitlab.com, stackoverflow.com
- News/content: medium.com, substack.com
- Crypto block explorers: etherscan.io, arbiscan.io, basescan.org, bscscan.com, polygonscan.com, ftmscan.com, snowtrace.io, avascan.info

**Note**: The base score for comments with URLs starts at 0.2 (low risk). Comments without URLs receive a score of 0.2 (positive signal - no URLs is good). Non-comment publications receive a neutral score of 0.5. URL normalization removes tracking parameters (utm\_\*, fbclid, etc.) before comparison.

### 5. Velocity Risk (Weight: 10% without IP, 8% with IP)

Measures how frequently an author is publishing to detect burst spam behavior. Queries both engine and indexer databases to count publications by the author's **signature public key** in the last hour and last 24 hours. Counts from both sources are **summed** to capture total network-wide activity.

**Note**: Indexer only tracks posts and replies. Votes, edits, and moderations are counted from engine data only.

The velocity risk score is the **maximum** of three checks:

1. **Per-type velocity**: Publications of the current type vs type-specific thresholds
2. **Aggregate velocity**: Total publications across ALL types vs aggregate thresholds
3. **Cross-type penalty**: If another publication type has higher velocity, blend 50% of that risk

#### Per-Type Thresholds

Different publication types have different acceptable rates:

**Post thresholds (comments without parentCid):**

| Posts/Hour | Risk Score | Description          |
| ---------- | ---------- | -------------------- |
| 0-2        | 0.10       | Normal posting rate  |
| 3-5        | 0.40       | Elevated rate        |
| 6-8        | 0.70       | Suspicious rate      |
| 12+        | 0.95       | Likely automated/bot |

**Reply thresholds (comments with parentCid):**

| Replies/Hour | Risk Score | Description          |
| ------------ | ---------- | -------------------- |
| 0-5          | 0.10       | Normal posting rate  |
| 6-10         | 0.40       | Elevated rate        |
| 11-15        | 0.70       | Suspicious rate      |
| 25+          | 0.95       | Likely automated/bot |

**Vote thresholds:**

| Votes/Hour | Risk Score | Description          |
| ---------- | ---------- | -------------------- |
| 0-20       | 0.10       | Normal voting rate   |
| 21-40      | 0.40       | Elevated rate        |
| 41-60      | 0.70       | Suspicious rate      |
| 100+       | 0.95       | Likely automated/bot |

**Comment edit thresholds:**

| Edits/Hour | Risk Score | Description          |
| ---------- | ---------- | -------------------- |
| 0-3        | 0.10       | Normal editing rate  |
| 4-5        | 0.40       | Elevated rate        |
| 6-10       | 0.70       | Suspicious rate      |
| 15+        | 0.95       | Likely automated/bot |

**Comment moderation thresholds:**

| Moderations/Hour | Risk Score | Description            |
| ---------------- | ---------- | ---------------------- |
| 0-5              | 0.10       | Normal moderation rate |
| 6-10             | 0.40       | Elevated rate          |
| 11-15            | 0.70       | Suspicious rate        |
| 25+              | 0.95       | Likely automated/bot   |

#### Aggregate Velocity Thresholds

Tracks total publications across ALL types combined. This catches authors who spread activity across multiple types to stay under individual thresholds.

| Total Publications/Hour | Risk Score | Description          |
| ----------------------- | ---------- | -------------------- |
| 0-25                    | 0.10       | Normal activity      |
| 26-50                   | 0.40       | Elevated activity    |
| 51-80                   | 0.70       | Suspicious activity  |
| 150+                    | 0.95       | Likely automated/bot |

**Example**: An author with 5 posts + 10 replies + 40 votes + 5 edits + 5 moderations = 65 total/hour would be flagged as SUSPICIOUS (0.70) even if each individual type is within normal limits.

#### Cross-Type Velocity Penalty

When evaluating a publication, if another publication type has higher velocity risk, 50% of the difference is blended into the current type's score.

**Formula**: `penalizedScore = currentScore + (otherMaxScore - currentScore) × 0.5`

**Example**: An author submitting a new post (1 post/hr = NORMAL 0.10) but has 150 votes/hr (BOT_LIKE 0.95):

- Per-type post score: 0.10
- Vote velocity score: 0.95
- Cross-type penalty: `0.10 + (0.95 - 0.10) × 0.5 = 0.525`

This ensures that suspicious behavior in one publication type affects the risk assessment of other types.

#### Effective Rate Calculation

The effective rate for each check is the maximum of:

- Publications of that type in the last hour
- Average publications per hour over the last 24 hours

**Note**: Subplebbit edits are not tracked for velocity as they are administrative actions.

**Replay attack protection**: The `/evaluate` endpoint rejects duplicate publications by checking `publication.signature.signature` before insertion. If the same publication is submitted multiple times, subsequent attempts return a 409 Conflict error. This prevents malicious subplebbits from artificially inflating an author's velocity count.

### 6. IP Risk (Weight: 0% without IP, 20% with IP)

When available (after iframe access), evaluates the author's IP address characteristics.

| IP Type     | Risk Score | Description           |
| ----------- | ---------- | --------------------- |
| Residential | 0.20       | Normal user IP        |
| Datacenter  | 0.70       | Often used for bots   |
| VPN         | 0.75       | Anonymizing service   |
| Proxy       | 0.85       | Anonymizing service   |
| Tor         | 0.95       | Highest anonymization |

**Note**: IP intelligence is best-effort and can have false positives. Use for informational purposes and rejection decisions only, not for auto-approval.

### 7. Network Ban History (Weight: 10% without IP, 8% with IP)

Evaluates the author's ban history across all indexed subplebbits, scaled by community breadth. Only **active** bans are counted (where `banExpiresAt >= current_time`); expired bans are ignored.

This factor uses data collected by the indexer, which:

- Subscribes to subplebbits discovered via the `/evaluate` API or `author.previousCommentCid` chains
- Tracks `CommentUpdate.author.subplebbit.banExpiresAt` to detect bans
- Aggregates ban data across all indexed subplebbits

**Formula** — two additive components, capped at 1.0:

```
cleanSubs = distinctSubs - activeBans

// Component 1: Ban severity (square root amplifies even small ban ratios)
banSeverity = activeBans >= distinctSubs ? 1.0
            : activeBans === 0            ? 0
            : sqrt(activeBans / distinctSubs)

// Component 2: Limited-community trust penalty (diminishing returns)
// Starts at 0.4 for 0 clean subs, drops to 0 around 15 clean subs
trustPenalty = max(0, 0.4 - 0.1 * log2(1 + cleanSubs))

score = min(1.0, banSeverity + trustPenalty)
```

**No posting history**: Factor is skipped (weight redistributed).

**Clean records (no active bans):**

| Subs | Bans | Clean | banSev | trustPen | Score | Meaning                         |
| ---- | ---- | ----- | ------ | -------- | ----- | ------------------------------- |
| 1    | 0    | 1     | 0      | 0.30     | 0.30  | 1 clean sub — weak trust signal |
| 3    | 0    | 3     | 0      | 0.20     | 0.20  | Some cross-sub presence         |
| 7    | 0    | 7     | 0      | 0.10     | 0.10  | Decent clean record             |
| 15   | 0    | 15    | 0      | 0.00     | 0.00  | Strong clean record             |
| 20   | 0    | 20    | 0      | 0.00     | 0.00  | Very strong clean record        |

**Mixed records (clean + active bans):**

| Subs | Bans | Clean | banSev | trustPen | Score | Meaning                  |
| ---- | ---- | ----- | ------ | -------- | ----- | ------------------------ |
| 20   | 1    | 19    | 0.22   | 0.00     | 0.22  | 1 ban in 20 — minor slip |
| 20   | 5    | 15    | 0.50   | 0.00     | 0.50  | 5 bans in 20             |
| 20   | 10   | 10    | 0.71   | 0.05     | 0.76  | Half banned              |
| 20   | 15   | 5     | 0.87   | 0.14     | 1.00  | Mostly banned (capped)   |
| 20   | 20   | 0     | 1.00   | —        | 1.00  | All banned               |
| 10   | 1    | 9     | 0.32   | 0.07     | 0.38  | 1 ban in 10              |
| 10   | 3    | 7     | 0.55   | 0.10     | 0.65  | 3 bans in 10             |
| 10   | 5    | 5     | 0.71   | 0.14     | 0.85  | Half banned              |
| 10   | 10   | 0     | 1.00   | —        | 1.00  | All banned               |
| 5    | 1    | 4     | 0.45   | 0.17     | 0.62  | 1 ban in 5               |
| 5    | 2    | 3     | 0.63   | 0.20     | 0.83  | 2 bans in 5              |
| 5    | 3    | 2     | 0.77   | 0.24     | 1.00  | 3 bans in 5 (capped)     |
| 3    | 1    | 2     | 0.58   | 0.24     | 0.82  | 1 ban in 3               |
| 3    | 2    | 1     | 0.82   | 0.30     | 1.00  | 2 bans in 3 (capped)     |
| 1    | 1    | 0     | 1.00   | —        | 1.00  | Banned from only sub     |

**Rationale**: The formula captures two signals: (1) ban severity scales with the square root of the ban ratio, so even 1 ban in 20 subs produces a non-trivial score, and (2) authors with more clean community presence get a diminishing trust bonus, reaching full trust around 15 clean subplebbits. Expired bans are ignored since the author has served their time. "No bans" is only a positive signal if the user has actually participated somewhere — for users with no posting history, this factor is skipped entirely.

### 8. ModQueue Rejection Rate (Weight: 6% without IP, 4% with IP)

Evaluates what percentage of the author's modQueue submissions were rejected.

The indexer tracks `subplebbit.modQueue.pendingApproval` entries and detects resolution by:

- Checking if the comment appears in regular pages (accepted)
- Checking if a full `CommentUpdate` can be fetched (accepted)
- If neither, the submission was rejected

| Rejection Rate | Risk Score | Description                           |
| -------------- | ---------- | ------------------------------------- |
| No data        | —          | Factor skipped (weight redistributed) |
| 0-10%          | 0.10       | Consistently approved                 |
| 10-30%         | 0.30       | Mostly approved                       |
| 30-50%         | 0.50       | Mixed history                         |
| 50-70%         | 0.70       | Frequently rejected                   |
| 70%+           | 0.90       | Usually rejected (high risk)          |

**Rationale**: Authors whose submissions are frequently rejected by moderators are more likely to be spamming.

### 9. Network Removal Rate (Weight: 8% without IP, 8% with IP)

Evaluates what percentage of the author's comments have been removed across all indexed subplebbits.

This includes:

- `CommentUpdate.removed = true` (mod removal)
- `CommentUpdate.approved = false` (disapproved)
- `CommentUpdate` fetch failures (likely purged/banned)

| Removal Rate | Risk Score | Description                           |
| ------------ | ---------- | ------------------------------------- |
| No data      | —          | Factor skipped (weight redistributed) |
| 0-5%         | 0.10       | Rarely removed                        |
| 5-15%        | 0.30       | Occasionally removed                  |
| 15-30%       | 0.50       | Moderate removal rate                 |
| 30-50%       | 0.70       | Frequently removed                    |
| 50%+         | 0.90       | Mostly removed (high risk)            |

**Rationale**: Authors whose content is frequently removed are likely posting inappropriate content.

### 10. Social Verification (Weight: 8% with/without IP)

Evaluates whether the author has verified via OAuth sign-in. This factor provides a **trust signal** - verified users receive lower risk scores.

**Behavior:**

- **OAuth disabled** (no enabled providers): Factor is **skipped** (weight redistributed)
- **OAuth enabled, no verification**: Score = **1.0** (high risk)
- **OAuth enabled, verified**: Score based on provider credibility (0.03 - 0.66)

**Provider Credibility Weights:**

Different providers have different credibility based on their verification strength:

| Provider  | Credibility | Rationale                                  |
| --------- | ----------- | ------------------------------------------ |
| google    | 1.0         | Phone verification, strong abuse detection |
| github    | 1.0         | Email required, developer-focused          |
| twitter   | 0.85        | Phone/email verification                   |
| discord   | 0.7         | Email required, bots common                |
| tiktok    | 0.6         | Phone typically required                   |
| reddit    | 0.6         | Email required, common bots                |
| yandex    | 0.5         | Less strict verification                   |
| (unknown) | 0.5         | Default for new/unrecognized providers     |

**Diminishing Returns - Multi-Author Reuse:**

When the same OAuth account is linked to multiple authors (identity sharing), each subsequent author receives reduced benefit:

| Author # | Benefit Factor | Description                   |
| -------- | -------------- | ----------------------------- |
| 1st      | 100% (1/√1)    | Full credibility              |
| 2nd      | 71% (1/√2)     | Reduced - identity shared     |
| 3rd      | 58% (1/√3)     | Further reduced               |
| 4th      | 50% (1/√4)     | Continues diminishing with √n |

**Multiple Service Decay:**

When an author verifies via multiple providers, each additional provider contributes less:

| Provider # | Contribution | Description                  |
| ---------- | ------------ | ---------------------------- |
| 1st        | 100%         | Full credibility             |
| 2nd        | 70%          | 30% decay                    |
| 3rd        | 49%          | Continues at 70% of previous |

Combined credibility is capped at **2.5** maximum.

**Score Calculation:**

Quadratic formula: `score = max(0, 1 - 0.75c + 0.15c²)` where `c` = combined credibility

| Verification State         | Credibility | Score     |
| -------------------------- | ----------- | --------- |
| OAuth disabled             | -           | (skipped) |
| No verification            | 0           | 1.0       |
| Single weak provider (0.5) | 0.5         | 0.66      |
| Single strong provider     | 1.0         | 0.40      |
| Strong + weak (with decay) | 1.35        | 0.26      |
| Two strong providers       | 1.7         | 0.15      |
| Three strong providers     | 2.19        | 0.07      |
| Maximum credibility        | 2.5         | 0.03      |

**Rationale**: OAuth verification provides a trust signal. Users who have verified with reputable providers are less likely to be spam bots or bad actors. The diminishing returns prevent gaming via identity sharing or mass verification.

### 11. Wallet Activity (Weight: 6% with/without IP)

Evaluates whether the author has a verified wallet with on-chain transaction history. Uses `eth_getTransactionCount` (nonce) as a proxy for wallet age and activity. A wallet with many outgoing transactions is almost certainly older and more active.

**Behavior:**

- **No wallet or nonce=0**: Factor is **skipped** (weight redistributed)
- **Wallet with transactions**: Score based on nonce tier (lower = more trust)

**Data Source:**

- Standard RPC `eth_getTransactionCount` call — works on any EVM node
- For ENS/domain wallet addresses, the name is first resolved to a hex address via the ETH chain provider
- Nonce is fetched on each wallet's declared chain (via `plebbit.chainProviders[chainTicker]`)
- Wallets whose chain has no configured RPC provider are skipped
- RPC failures are handled gracefully (wallet treated as nonce=0)

**Nonce-Based Scoring:**

| Nonce (outgoing tx count) | Risk Score | Description                          |
| ------------------------- | ---------- | ------------------------------------ |
| 0 (or no wallet)          | (skipped)  | Factor skipped, weight redistributed |
| 1–10                      | 0.35       | Some activity, modest trust          |
| 11–50                     | 0.25       | Moderate activity                    |
| 51–200                    | 0.15       | Strong activity                      |
| 200+                      | 0.10       | Very strong activity                 |

**Strict 1-to-1 Wallet-Author Enforcement:**

Each wallet can only be associated with **one** author public key. This prevents an attacker from creating one active wallet and reusing it across many sybil accounts.

| Wallet State                        | Behavior                   |
| ----------------------------------- | -------------------------- |
| Used only by current author         | Counts normally            |
| Used by any other author public key | Ignored entirely           |
| All wallets discarded               | Factor skipped (weight: 0) |

The enforcement queries all publication tables (`comments`, `votes`, `commentEdits`, `commentModerations`) for any record with a different `publicKey` that has the same wallet address.

**Best Wallet Wins:**

If an author has multiple valid wallets (across different chains), the one with the highest nonce is used for scoring.

**Rationale**: Wallets with on-chain transaction history demonstrate real blockchain usage. The economic cost of creating active wallets (gas fees for real transactions) makes this a meaningful trust signal. The strict 1-to-1 mapping prevents wallet-sharing sybil attacks.

## Weight Distribution

Weights are redistributed based on whether IP information is available:

| Factor                     | Without IP | With IP  |
| -------------------------- | ---------- | -------- |
| Comment Content/Title Risk | 14%        | 10%      |
| Comment URL/Link Risk      | 12%        | 10%      |
| Velocity Risk              | 10%        | 8%       |
| Account Age                | 14%        | 10%      |
| Karma Score                | 12%        | 8%       |
| IP Risk                    | 0%         | 20%      |
| Network Ban History        | 10%        | 8%       |
| ModQueue Rejection Rate    | 6%         | 4%       |
| Network Removal Rate       | 8%         | 8%       |
| Social Verification        | 8%         | 8%       |
| Wallet Activity            | 6%         | 6%       |
| **Total**                  | **100%**   | **100%** |

**Note**: Total weights sum to 100% when all factors are active. When factors are skipped (weight=0), active factors receive proportionally redistributed weights that still sum to 100% (see Weight Redistribution below).

## Weight Redistribution

Some factors may be **skipped** (return `weight: 0`) when their required data is unavailable or when they don't apply to the publication type:

| Factor              | Conditions for Skipping                                            |
| ------------------- | ------------------------------------------------------------------ |
| Velocity            | Publication is `subplebbitEdit`                                    |
| IP Risk             | No IP intelligence data available (user hasn't accessed iframe)    |
| Content/Title Risk  | Publication is not a comment (vote, edit, moderation)              |
| URL/Link Risk       | Publication is not a comment (vote, edit, moderation)              |
| Social Verification | OAuth is completely disabled (no enabled providers)                |
| Wallet Activity     | No wallets, no transaction data, nonce=0, or all wallets discarded |

When a factor is skipped, its weight is **proportionally redistributed** to the remaining active factors. This ensures:

1. The relative importance between active factors is preserved
2. Effective weights always sum to 1.0
3. Skipped factors don't create "dead weight" in the calculation

### Redistribution Formula

For each active factor:

```
effectiveWeight = originalWeight / Σ(activeWeights)
```

### Example: Vote Publication (Multiple Factors Skipped, OAuth Enabled, No Wallet)

For vote publications, Content Risk and URL Risk are skipped. Wallet Activity is also skipped (no wallet data).

| Factor              | Original Weight | Effective Weight |
| ------------------- | --------------- | ---------------- |
| Content Risk        | 14%             | **0% (skipped)** |
| URL Risk            | 12%             | **0% (skipped)** |
| Velocity            | 10%             | 14.7%            |
| Account Age         | 14%             | 20.6%            |
| Karma               | 12%             | 17.6%            |
| Ban History         | 10%             | 14.7%            |
| ModQueue Rejection  | 6%              | 8.8%             |
| Removal Rate        | 8%              | 11.8%            |
| Social Verification | 8%              | 11.8%            |
| Wallet Activity     | 6%              | **0% (skipped)** |
| **Total**           | 100%            | **100%**         |

Active total: 10% + 14% + 12% + 10% + 6% + 8% + 8% = 68%

Calculation: `0.10 / 0.68 ≈ 0.147` for Velocity (10% original / 68% active total)

## Score Calculation

The final score is computed as:

```
finalScore = Σ(factor.score × factor.effectiveWeight)
```

Where `effectiveWeight` is the redistributed weight after accounting for skipped factors. Each factor produces a score between 0.0 and 1.0, which is multiplied by its effective weight. The weighted scores sum to the final score (since effective weights sum to 1.0).

## Challenge Tier System

The risk score is mapped to a challenge tier that determines what verification the user must complete:

| Risk Score Range                           | Tier                | Action                    |
| ------------------------------------------ | ------------------- | ------------------------- |
| 0.0 - autoAcceptThreshold                  | `auto_accept`       | No challenge, auto-accept |
| autoAcceptThreshold - captchaOnlyThreshold | `captcha_only`      | CAPTCHA (Turnstile) only  |
| captchaOnlyThreshold - autoRejectThreshold | `captcha_and_oauth` | CAPTCHA + OAuth sign-in   |
| autoRejectThreshold - 1.0                  | `auto_reject`       | No challenge, auto-reject |

### Default Thresholds

| Threshold              | Default Value | Description                                               |
| ---------------------- | ------------- | --------------------------------------------------------- |
| `autoAcceptThreshold`  | 0.2           | Scores below this are auto-accepted without any challenge |
| `captchaOnlyThreshold` | 0.4           | Scores between auto-accept and this get CAPTCHA only      |
| `autoRejectThreshold`  | 0.8           | Scores at or above this are auto-rejected                 |

### Challenge Tier Behavior

**`auto_accept`**: The session is immediately marked as completed. The user doesn't need to solve any challenge.

**`captcha_only`**: The user must complete a CAPTCHA (Cloudflare Turnstile). Once verified, the session is marked as completed.

**`captcha_and_oauth`**: The user must complete both:

1. CAPTCHA (Turnstile) - verifies they are human
2. OAuth sign-in - verifies they have a real social account

The CAPTCHA must be completed first, then OAuth buttons become available. This two-step verification provides stronger assurance for medium-risk submissions.

**`auto_reject`**: The session is immediately marked as failed. The user cannot complete the challenge.

### OAuth Provider Filtering

When `captcha_and_oauth` tier is triggered, the system checks if the user has previously verified via OAuth with any providers. If so, **only providers they haven't used before** are shown. This prevents users from repeatedly using the same OAuth account.

If the user has already used all available OAuth providers, the tier is downgraded to `captcha_only`.

### Fallback Behavior

The system gracefully degrades based on available providers:

| Configured Providers | captcha_only Tier              | captcha_and_oauth Tier         |
| -------------------- | ------------------------------ | ------------------------------ |
| Both CAPTCHA + OAuth | Turnstile CAPTCHA              | Turnstile + OAuth              |
| CAPTCHA only         | Turnstile CAPTCHA              | Downgrades to Turnstile only   |
| OAuth only           | OAuth sign-in                  | Downgrades to OAuth only       |
| Neither              | Error (no challenge available) | Error (no challenge available) |

## Limitations

1. **IP Intelligence Accuracy**: VPN/proxy detection is imperfect. Residential IPs can be misclassified.

2. **New Subplebbit Limitation**: For new subplebbits, `author.subplebbit` data may be empty for all users initially.

3. **Content Similarity Scope**: Uses word-level Jaccard similarity (60% threshold). May miss semantically similar content that uses different words. Substring matching helps catch some variations but sophisticated paraphrasing may evade detection.

4. **Comment-Only Content Analysis**: Content/title similarity analysis only applies to comments (posts and replies). Other publication types (votes, edits, moderations) receive a neutral score for this factor.

## Future Improvements

- Challenge completion history tracking
- Machine learning-based content analysis
- Moderation action history integration
